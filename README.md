# ğŸ›’ CLV Prediction

## ğŸ“ŒBusiness Problem
Predict **6-month Customer Lifetime Value (CLV)** for each customer based on historical transaction behavior from the
[Online Retail II dataset](https://archive.ics.uci.edu/dataset/502/online+retail+ii).

The objective is to:
- Identify high-value customers for retention and marketing
- Build a **production-ready ML system** with experiment tracking, orchestration, monitoring, and reproducibility

---

## CLV Definition (Regression Target)

**CLV_6M** = Total revenue generated by a customer in the **6 months after** a fixed reference date  
(reference date: `2011-06-01`)

The modeling dataset is a **single snapshot**, customer-level table created using only historical data prior to the cutoff.

---

## Feature Summary

| Feature           | Description                                 |
|-------------------|---------------------------------------------|
| recency_days      | Days since last purchase                    |
| frequency         | Number of invoices                          |
| total_revenue     | Historical spend                            |
| avg_order_value   | Average revenue per order                   |
| tenure_days       | Customer lifetime                           |
| active_months     | Number of months with activity              |
| purchase_velocity | Purchases per active month                  |
| avg_gap_days      | Average gap between purchases               |
| std_gap_days      | Purchase interval variability               |
| clv_6m            | Target variable                             |

---

## ğŸ’¡Project Background

This repo extends a machine learning notebook into a full MLOps workflow following the [Machine Learning Zoomcamp](https://github.com/DataTalksClub/machine-learning-zoomcamp) Capstone Project guidelines. 

It covers:
- Data preparation and EDA
- Training and selecting the best model
- Experiment tracking using MLFlow (additional)
- Serving via FastAPI
- Monitoring using Prometheus and Grafana (additional)
- Docker for reproducibility
- Cloud deployment using GCP
- Applying best practices (additional):
    - Unit tests
    - Integration test
    - Linter and code formatter
    - Makefile
    - Pre-commit hooks
    - CI/CD pipeline

---

## âš™ï¸ Tech Stack & Architecture

| Tool                    | Purpose                                |
| ----------------------- | -------------------------------------- |
| Pandas, scikit-learn    | Data handling and modeling             |
| MLflow                  | Experiment tracking and model registry |
| Prefect                 | Orchestration                          |
| FastAPI, Docker         | Serving and containerization           |
| Prometheus + Grafana    | Monitoring metrics and dashboards      |
| GitHub Actions          | CI/CD pipeline                         |
| Black + Ruff            | Code quality                           |
| GCP Cloud Run           | Cloud deplyment                        |

System flow:
`online_retail_II.xlsx` â†’ `train.py` (MLflow logs & registered the best model) â†’ `FastAPI app` (serves model) â†’ `Docker + docker-compose` â†’ Prometheus scrapes `/metrics` â†’ Grafana dashboards visualize.

![Diagram](images/diagram.png)

---

## ğŸ¤– Model Training

Multiple models were trained and evaluated:
- Linear Regression
- Random Forest egressor
- Gradient Boosting Regressor

The best-performing pipeline was registered in `MLflow` with metrics logged and artifacts stored.

To train model:
```
make run
```

This will:
- Train multiple models
- Log metrics and artifacts (MLflow)
- Export the best model to models/

A trained and validated model pipeline was exported to the `models/` directory and is treated as the production artifact.
```
models/
â””â”€â”€ clv_linear_regression/
```

This model artifact is used directly by:
- predict.py
- FastAPI inference service
- Dockerized deployment
- Cloud Run deployment

---
## ğŸ”¬ Experiment Tracking

This project uses MLFlow to track experiment and model registry.
To view experiment:
```
mlflow ui
```

Open: http://localhost:5000

You can explore:
- Experiments
- Runs
- Registered models

![MLFlow](images/mlflow.png)


---
## ğŸ¯Workflow Orchestration (Prefect)

Prefect shows how training can be orchestrated as a batch workflow.

To start the Prefect UI:
```
prefect server start
```

To run the training flow:
```
python src/train_flow.py
```
This:
- Executes the training pipeline
- Adds retries and observability
- Simulates a production batch job

![Prefect](images/prefect.png)

---

## ğŸ§ªReproducibility

All experiments are reproducible via:
- Locked dependencies (`uv.lock`)
- Training logic is exported to `train.py`
- Prediction logic is in `predict.py`
- MLflow is used for experiment tracking and model registry
- SQLite backend ensures local reproducibility
- Dataset is publicly available on Kaggle with download instructions in the notebook and README

---

## ğŸ“ˆ Monitoring
Prometheus and Grafana are integrated for live monitoring:
- Request count, latency, and response size
- Dashboard panels for operational metrics
- Docker Compose orchestrates all services

---

## â˜ï¸ Cloud Deployment (GCP)

The FastAPI inference service is deployed on **Google Cloud Run**.

**Build Docker Image**
```bash
gcloud builds submit \
  --tag asia-southeast2-docker.pkg.dev/clv-prediction-479007/clv-repo/clv-api:latest

gcloud run deploy clv-api \
  --image asia-southeast2-docker.pkg.dev/clv-prediction-479007/clv-repo/clv-api:latest \
  --platform managed \
  --allow-unauthenticated \
  --port 8000
```
**Live Service**

- Base URL: [https://clv-api-696779814192.asia-southeast2.run.app/](https://clv-api-696779814192.asia-southeast2.run.app/)
- Health check: `/health`
- Prediction endpoint: `/predict`

**Example Request**
```bash
curl -X POST "https://clv-api-696779814192.asia-southeast2.run.app/predict" \
  -H "Content-Type: application/json" \
  -d @test.json
```

![GCP Predict](images/gcp_predict.png)

![GCP Monitoring](images/gcp_monitoring.png)

---

## ğŸ‘Best Practices (Testing, Linting, and Quality Checks)

This project follows industry best practices:
- Unit & integration tests:
```
make test
```

![Pytest](images/pytest.png)

- Linting & formatting:
```
make format
make lint
```

![Black 1](images/black_1.png)

![Black 2](images/black_2.png)

![Lint](images/lint.png)

- Pre-commit hooks:
```
pre-commit install
```
- CI/CD:
    - GitHub Actions automatically run linting and tests on every push and PR.

---

## âš“ Project Usage Guide

This section describes how to set up, train, orchestrate, test, and serve the CLV prediction system following a production-oriented ML workflow.

### 1. Clone the Repository
```bash
git clone https://github.com/deedeepratiwi/clv-prediction.git
cd clv-prediction
```

### 2. Environment & Dependency Setup

This project uses uv for fast, reproducible dependency management.
```bash
uv venv
uv sync --locked
```

Activate the environment if needed:
```bash
source .venv/bin/activate  # Linux / macOS
.venv\Scripts\activate     # Windows
```

### 3. Quick Sanity Check (Local Prediction)

Before running any services, verify inference works.
```
python src/test_predict.py
```

Expected output:
```
Predicted_clv_6m: 8051.121461205699
```

### 4. Run the API Locally

Start the FastAPI inference service:
```
make api
```
Available endpoints:
- `POST /predict` â†’ Generate CLV predictions
- `GET /health` â†’ Service health check
- `GET /metrics` â†’ Prometheus metrics

API docs:
`http://localhost:8000/docs` 

Example request:
```
curl -X POST "http://localhost:8000/predict" \
     -H "Content-Type: application/json" \
     -d @./test.json
```

Expected result:
```
{"predicted_clv_6m":8051.121461205699}
```

### 5. Run the Full Stack (Docker + Monitoring)

This is the production-style setup.
```
docker-compose up --build
```
This starts:
- FastAPI services
- Prometheus (metrics collection)
- Grafana (monitoring dashboard)

### 6. Monitoring & Observability

- **FastAPI**: http://localhost:8000/docs

![FastAPI App](images/predict.png)

- **Prometheus**: http://localhost:9090
    - Example metric: `clv_request_latency_seconds_bucket`

![Prometheus](images/prometheus.png)

- **Grafana**: http://localhost:3000
    - Login: `admin` / `admin`
    - Add new connection: http://prometheus:9090
    - Import dashboard JSON from `monitoring/`

![Grafana FastAPI Monitoring](images/grafana.png)

---

## ğŸ“ Project Structure
```
.
â”œâ”€â”€ .github/workflows/
â”‚ â””â”€â”€ ci.yaml
â”œâ”€â”€ data/                               # Ignored
â”‚ â”œâ”€â”€ raw/ 
â”‚ â””â”€â”€ processed/
â”œâ”€â”€ images/
â”œâ”€â”€ monitoring/
â”‚ â””â”€â”€ prometheus.yml
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ 01_eda_raw_data.ipynb
â”‚ â”œâ”€â”€ 02_target_contruction.ipynb
â”‚ â”œâ”€â”€ 03_feature_engineering.ipynb
â”‚ â””â”€â”€ 04_train_baseline.ipynb
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ api.py                            # FastAPI service
â”‚ â”œâ”€â”€ train.py                          # Model training + MLflow logging
â”‚ â”œâ”€â”€ predict.py                        # Inference logic
â”‚ â””â”€â”€ train_flow.py                     # Prefect orchestration
â”œâ”€â”€ tests/
â”‚ â”œâ”€â”€ test_api.py                       # Integration test
â”‚ â””â”€â”€ test_predictor.py                 # Unit test
â”œâ”€â”€ models/                             # Ignored (artifacts)
â”œâ”€â”€ docker-compose.yml 
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ Makefile
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirement.txt
â””â”€â”€ README.md
```

---
